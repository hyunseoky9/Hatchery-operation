{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ac9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement intel-extension-for-pytorch (from versions: none)\n",
      "ERROR: No matching distribution found for intel-extension-for-pytorch\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9da20b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebddacf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "<bound method NDFrame.head of              x1        x2        x3        x4        x5        x6  \\\n",
      "0      0.249496  1.651955 -0.518763  0.186928  0.347909 -0.286217   \n",
      "1      0.164447  0.130632  0.729240 -0.011466 -0.333234 -0.997897   \n",
      "2     -0.494170  1.240444  1.433147  0.151573  0.001301 -0.645137   \n",
      "3      1.693532  0.500309 -0.518393 -1.146228 -0.304018  0.492199   \n",
      "4      0.640281 -1.470265 -0.521338  1.002532 -1.641815 -1.618025   \n",
      "...         ...       ...       ...       ...       ...       ...   \n",
      "99995 -0.569228  0.464088 -0.177436 -1.567349  0.186987 -1.317814   \n",
      "99996 -0.768586  1.473541 -1.370753  0.167335 -1.615549  0.419048   \n",
      "99997  0.275200  1.346814  0.277645 -1.627968 -1.512860  0.507975   \n",
      "99998  0.506362  0.151971 -1.478689  1.199874  1.702765 -0.689720   \n",
      "99999  0.513383  1.655695 -0.716390 -1.013162  1.262534  0.992421   \n",
      "\n",
      "                y1         y2  \n",
      "0       404.006021  26.117891  \n",
      "1       234.866294  41.490010  \n",
      "2       363.410767  56.836420  \n",
      "3        11.803386  21.565830  \n",
      "4      3291.243465  34.639558  \n",
      "...            ...        ...  \n",
      "99995     2.335926   9.602242  \n",
      "99996   381.864060  14.294863  \n",
      "99997     8.370706   9.235833  \n",
      "99998  5482.490651  15.714494  \n",
      "99999    25.578401  11.842325  \n",
      "\n",
      "[100000 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from fnapproxto import fnapproxto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# import csv data with pd\n",
    "data = pd.read_csv('generated_dataset.csv')\n",
    "mean = np.array(data).mean(axis=0, keepdims=True)\n",
    "std = np.array(data).std(axis=0, keepdims=True)\n",
    "xvarnum = 6\n",
    "for i in range(xvarnum):\n",
    "    data.iloc[:, i] = (data.iloc[:, i] - mean[0][i]) / std[0][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "800bcd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "(100000, 8)\n",
      "Epoch 1\n",
      "Test loss: 13659144.0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from fnapproxto import fnapproxto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# import csv data with pd\n",
    "data = pd.read_csv('generated_dataset.csv')\n",
    "mean = np.array(data).mean(axis=0, keepdims=True)\n",
    "std = np.array(data).std(axis=0, keepdims=True)\n",
    "xvarnum = 6\n",
    "for i in range(xvarnum):\n",
    "    data.iloc[:, i] = (data.iloc[:, i] - mean[0][i]) / std[0][i]\n",
    "\n",
    "print(data.shape)\n",
    "# show first six rows of data\n",
    "#print(data.head())\n",
    "\n",
    "train_data = data.sample(frac=0.8, random_state=0)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "# batch\n",
    "batch_num = 10\n",
    "\n",
    "# separate the data into batches then to tensors \n",
    "def create_batches(data, num_batches):\n",
    "    data = np.array(data)\n",
    "    np.random.shuffle(data)\n",
    "    batch_size = len(data) // num_batches\n",
    "    batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    return [(torch.tensor(batch[:,0:6], dtype=torch.float32),torch.tensor(batch[:,-2:], dtype=torch.float32)) for batch in batches]\n",
    "\n",
    "\n",
    "batched_training_data = create_batches(train_data, batch_num)\n",
    "batched_test_data = create_batches(test_data, 1)\n",
    "\n",
    "\n",
    "\n",
    "# initiate the model\n",
    "state_size = 6\n",
    "action_size = 2\n",
    "hidden_size = 31\n",
    "learning_rate = 0.0001\n",
    "fnapprox = fnapproxto(state_size=state_size, action_size=action_size, hidden_size=hidden_size, learning_rate=learning_rate).to(device)\n",
    "\n",
    "\n",
    "# run the training loops\n",
    "epochs = 1\n",
    "testloss = []\n",
    "for t in range(epochs):\n",
    "    fnapprox.train_model(batched_training_data,device)\n",
    "    if t % 100 == 0:\n",
    "        print(f\"Epoch {t+1}\")\n",
    "        loss = fnapprox.test_model(batched_test_data,device)\n",
    "        testloss.append(loss)\n",
    "        print(f\"Test loss: {loss}\\r\")\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68730193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71401\n",
      "Test loss: 3.9733469486236572\n",
      "Epoch 71501\n",
      "Test loss: 3.9729602336883545\n",
      "Epoch 71601\n",
      "Test loss: 3.972149133682251\n",
      "Epoch 71701\n",
      "Test loss: 3.9713077545166016\n",
      "Epoch 71801\n",
      "Test loss: 3.97056245803833\n",
      "Epoch 71901\n",
      "Test loss: 3.9699628353118896\n",
      "Epoch 72001\n",
      "Test loss: 3.9693171977996826\n",
      "Epoch 72101\n",
      "Test loss: 3.9687840938568115\n",
      "Epoch 72201\n",
      "Test loss: 3.968243360519409\n",
      "Epoch 72301\n",
      "Test loss: 3.9677555561065674\n",
      "Epoch 72401\n",
      "Test loss: 3.966158628463745\n",
      "Epoch 72501\n",
      "Test loss: 3.966482400894165\n",
      "Epoch 72601\n",
      "Test loss: 3.965475082397461\n",
      "Epoch 72701\n",
      "Test loss: 3.965581178665161\n",
      "Epoch 72801\n",
      "Test loss: 3.9645586013793945\n",
      "Epoch 72901\n",
      "Test loss: 3.964478015899658\n",
      "Epoch 73001\n",
      "Test loss: 3.9638988971710205\n",
      "Epoch 73101\n",
      "Test loss: 3.9626383781433105\n",
      "Epoch 73201\n",
      "Test loss: 3.962134838104248\n",
      "Epoch 73301\n",
      "Test loss: 3.9623007774353027\n",
      "Epoch 73401\n",
      "Test loss: 3.961516857147217\n",
      "Epoch 73501\n",
      "Test loss: 3.9607796669006348\n",
      "Epoch 73601\n",
      "Test loss: 3.960557460784912\n",
      "Epoch 73701\n",
      "Test loss: 3.9594297409057617\n",
      "Epoch 73801\n",
      "Test loss: 3.959503412246704\n",
      "Epoch 73901\n",
      "Test loss: 3.958292245864868\n",
      "Epoch 74001\n",
      "Test loss: 3.958177328109741\n",
      "Epoch 74101\n",
      "Test loss: 3.9576094150543213\n",
      "Epoch 74201\n",
      "Test loss: 3.956866502761841\n",
      "Epoch 74301\n",
      "Test loss: 3.9566493034362793\n",
      "Epoch 74401\n",
      "Test loss: 3.9554686546325684\n",
      "Epoch 74501\n",
      "Test loss: 3.955209493637085\n",
      "Epoch 74601\n",
      "Test loss: 3.954422950744629\n",
      "Epoch 74701\n",
      "Test loss: 3.954664945602417\n",
      "Epoch 74801\n",
      "Test loss: 3.953845262527466\n",
      "Epoch 74901\n",
      "Test loss: 3.9536163806915283\n",
      "Epoch 75001\n",
      "Test loss: 3.9528467655181885\n",
      "Epoch 75101\n",
      "Test loss: 3.9523487091064453\n",
      "Epoch 75201\n",
      "Test loss: 3.9517805576324463\n",
      "Epoch 75301\n",
      "Test loss: 3.9512410163879395\n",
      "Epoch 75401\n",
      "Test loss: 3.950638771057129\n",
      "Epoch 75501\n",
      "Test loss: 3.9505350589752197\n",
      "Epoch 75601\n",
      "Test loss: 3.95009446144104\n",
      "Epoch 75701\n",
      "Test loss: 3.9487030506134033\n",
      "Epoch 75801\n",
      "Test loss: 3.9486618041992188\n",
      "Epoch 75901\n",
      "Test loss: 3.9485585689544678\n",
      "Epoch 76001\n",
      "Test loss: 3.946899175643921\n",
      "Epoch 76101\n",
      "Test loss: 3.9470582008361816\n",
      "Epoch 76201\n",
      "Test loss: 3.946427345275879\n",
      "Epoch 76301\n",
      "Test loss: 3.9459071159362793\n",
      "Epoch 76401\n",
      "Test loss: 3.9454855918884277\n",
      "Epoch 76501\n",
      "Test loss: 3.9451422691345215\n",
      "Epoch 76601\n",
      "Test loss: 3.944359302520752\n",
      "Epoch 76701\n",
      "Test loss: 3.943864107131958\n",
      "Epoch 76801\n",
      "Test loss: 3.943514823913574\n",
      "Epoch 76901\n",
      "Test loss: 3.943249225616455\n",
      "Epoch 77001\n",
      "Test loss: 3.942345380783081\n",
      "Epoch 77101\n",
      "Test loss: 3.9416043758392334\n",
      "Epoch 77201\n",
      "Test loss: 3.941411018371582\n",
      "Epoch 77301\n",
      "Test loss: 3.940415620803833\n",
      "Epoch 77401\n",
      "Test loss: 3.9401586055755615\n",
      "Epoch 77501\n",
      "Test loss: 3.9406070709228516\n",
      "Epoch 77601\n",
      "Test loss: 3.9393045902252197\n",
      "Epoch 77701\n",
      "Test loss: 3.9392828941345215\n",
      "Epoch 77801\n",
      "Test loss: 3.9382243156433105\n",
      "Epoch 77901\n",
      "Test loss: 3.937347173690796\n",
      "Epoch 78001\n",
      "Test loss: 3.9370930194854736\n",
      "Epoch 78101\n",
      "Test loss: 3.9366683959960938\n",
      "Epoch 78201\n",
      "Test loss: 3.936103582382202\n",
      "Epoch 78301\n",
      "Test loss: 3.9356813430786133\n",
      "Epoch 78401\n",
      "Test loss: 3.9351937770843506\n",
      "Epoch 78501\n",
      "Test loss: 3.9345428943634033\n",
      "Epoch 78601\n",
      "Test loss: 3.9335265159606934\n",
      "Epoch 78701\n",
      "Test loss: 3.9331257343292236\n",
      "Epoch 78801\n",
      "Test loss: 3.932716131210327\n",
      "Epoch 78901\n",
      "Test loss: 3.9316155910491943\n",
      "Epoch 79001\n",
      "Test loss: 3.9312968254089355\n",
      "Epoch 79101\n",
      "Test loss: 3.9308085441589355\n",
      "Epoch 79201\n",
      "Test loss: 3.9302468299865723\n",
      "Epoch 79301\n",
      "Test loss: 3.9294238090515137\n",
      "Epoch 79401\n",
      "Test loss: 3.9287967681884766\n",
      "Epoch 79501\n",
      "Test loss: 3.9285848140716553\n",
      "Epoch 79601\n",
      "Test loss: 3.9281187057495117\n",
      "Epoch 79701\n",
      "Test loss: 3.9271812438964844\n",
      "Epoch 79801\n",
      "Test loss: 3.9267406463623047\n",
      "Epoch 79901\n",
      "Test loss: 3.926400899887085\n",
      "Epoch 80001\n",
      "Test loss: 3.9253859519958496\n",
      "Epoch 80101\n",
      "Test loss: 3.9245195388793945\n",
      "Epoch 80201\n",
      "Test loss: 3.924067974090576\n",
      "Epoch 80301\n",
      "Test loss: 3.923452377319336\n",
      "Epoch 80401\n",
      "Test loss: 3.923161745071411\n",
      "Epoch 80501\n",
      "Test loss: 3.9223735332489014\n",
      "Epoch 80601\n",
      "Test loss: 3.9219582080841064\n",
      "Epoch 80701\n",
      "Test loss: 3.9210636615753174\n",
      "Epoch 80801\n",
      "Test loss: 3.920452356338501\n",
      "Epoch 80901\n",
      "Test loss: 3.9199843406677246\n",
      "Epoch 81001\n",
      "Test loss: 3.919600009918213\n",
      "Epoch 81101\n",
      "Test loss: 3.918938398361206\n",
      "Epoch 81201\n",
      "Test loss: 3.918339729309082\n",
      "Epoch 81301\n",
      "Test loss: 3.917862892150879\n",
      "Epoch 81401\n",
      "Test loss: 3.917548894882202\n",
      "Epoch 81501\n",
      "Test loss: 3.9164483547210693\n",
      "Epoch 81601\n",
      "Test loss: 3.9165713787078857\n",
      "Epoch 81701\n",
      "Test loss: 3.9155242443084717\n",
      "Epoch 81801\n",
      "Test loss: 3.9151597023010254\n",
      "Epoch 81901\n",
      "Test loss: 3.913938283920288\n",
      "Epoch 82001\n",
      "Test loss: 3.913665533065796\n",
      "Epoch 82101\n",
      "Test loss: 3.912890672683716\n",
      "Epoch 82201\n",
      "Test loss: 3.912971019744873\n",
      "Epoch 82301\n",
      "Test loss: 3.911820411682129\n",
      "Epoch 82401\n",
      "Test loss: 3.9118781089782715\n",
      "Epoch 82501\n",
      "Test loss: 3.9112305641174316\n",
      "Epoch 82601\n",
      "Test loss: 3.9102933406829834\n",
      "Epoch 82701\n",
      "Test loss: 3.9097390174865723\n",
      "Epoch 82801\n",
      "Test loss: 3.9092726707458496\n",
      "Epoch 82901\n",
      "Test loss: 3.9089043140411377\n",
      "Epoch 83001\n",
      "Test loss: 3.907946825027466\n",
      "Epoch 83101\n",
      "Test loss: 3.9066312313079834\n",
      "Epoch 83201\n",
      "Test loss: 3.906757354736328\n",
      "Epoch 83301\n",
      "Test loss: 3.9063363075256348\n",
      "Epoch 83401\n",
      "Test loss: 3.905872344970703\n",
      "Epoch 83501\n",
      "Test loss: 3.905308485031128\n",
      "Epoch 83601\n",
      "Test loss: 3.904862880706787\n",
      "Epoch 83701\n",
      "Test loss: 3.9041385650634766\n",
      "Epoch 83801\n",
      "Test loss: 3.903644561767578\n",
      "Epoch 83901\n",
      "Test loss: 3.903266429901123\n",
      "Epoch 84001\n",
      "Test loss: 3.9029881954193115\n",
      "Epoch 84101\n",
      "Test loss: 3.9022328853607178\n",
      "Epoch 84201\n",
      "Test loss: 3.901843786239624\n",
      "Epoch 84301\n",
      "Test loss: 3.901615619659424\n",
      "Epoch 84401\n",
      "Test loss: 3.900519609451294\n",
      "Epoch 84501\n",
      "Test loss: 3.899787425994873\n",
      "Epoch 84601\n",
      "Test loss: 3.899693250656128\n",
      "Epoch 84701\n",
      "Test loss: 3.8991734981536865\n",
      "Epoch 84801\n",
      "Test loss: 3.89855694770813\n",
      "Epoch 84901\n",
      "Test loss: 3.8977725505828857\n",
      "Epoch 85001\n",
      "Test loss: 3.8970422744750977\n",
      "Epoch 85101\n",
      "Test loss: 3.896939754486084\n",
      "Epoch 85201\n",
      "Test loss: 3.8964765071868896\n",
      "Epoch 85301\n",
      "Test loss: 3.8957555294036865\n",
      "Epoch 85401\n",
      "Test loss: 3.8956034183502197\n",
      "Epoch 85501\n",
      "Test loss: 3.8945906162261963\n",
      "Epoch 85601\n",
      "Test loss: 3.8943839073181152\n",
      "Epoch 85701\n",
      "Test loss: 3.8939077854156494\n",
      "Epoch 85801\n",
      "Test loss: 3.89357852935791\n",
      "Epoch 85901\n",
      "Test loss: 3.8930015563964844\n",
      "Epoch 86001\n",
      "Test loss: 3.892279624938965\n",
      "Epoch 86101\n",
      "Test loss: 3.8909316062927246\n",
      "Epoch 86201\n",
      "Test loss: 3.891608476638794\n",
      "Epoch 86301\n",
      "Test loss: 3.8907687664031982\n",
      "Epoch 86401\n",
      "Test loss: 3.8901612758636475\n",
      "Epoch 86501\n",
      "Test loss: 3.8896045684814453\n",
      "Epoch 86601\n",
      "Test loss: 3.8890256881713867\n",
      "Epoch 86701\n",
      "Test loss: 3.8889553546905518\n",
      "Epoch 86801\n",
      "Test loss: 3.888362169265747\n",
      "Epoch 86901\n",
      "Test loss: 3.887608289718628\n",
      "Epoch 87001\n",
      "Test loss: 3.8877081871032715\n",
      "Epoch 87101\n",
      "Test loss: 3.886892557144165\n",
      "Epoch 87201\n",
      "Test loss: 3.8860366344451904\n",
      "Epoch 87301\n",
      "Test loss: 3.8855199813842773\n",
      "Epoch 87401\n",
      "Test loss: 3.885288953781128\n",
      "Epoch 87501\n",
      "Test loss: 3.8844375610351562\n",
      "Epoch 87601\n",
      "Test loss: 3.8842086791992188\n",
      "Epoch 87701\n",
      "Test loss: 3.8834311962127686\n",
      "Epoch 87801\n",
      "Test loss: 3.8831686973571777\n",
      "Epoch 87901\n",
      "Test loss: 3.8826851844787598\n",
      "Epoch 88001\n",
      "Test loss: 3.8822851181030273\n",
      "Epoch 88101\n",
      "Test loss: 3.8820061683654785\n",
      "Epoch 88201\n",
      "Test loss: 3.8814070224761963\n",
      "Epoch 88301\n",
      "Test loss: 3.8802480697631836\n",
      "Epoch 88401\n",
      "Test loss: 3.880262613296509\n",
      "Epoch 88501\n",
      "Test loss: 3.879990577697754\n",
      "Epoch 88601\n",
      "Test loss: 3.8794121742248535\n",
      "Epoch 88701\n",
      "Test loss: 3.8785359859466553\n",
      "Epoch 88801\n",
      "Test loss: 3.8785085678100586\n",
      "Epoch 88901\n",
      "Test loss: 3.877382755279541\n",
      "Epoch 89001\n",
      "Test loss: 3.876967191696167\n",
      "Epoch 89101\n",
      "Test loss: 3.8769147396087646\n",
      "Epoch 89201\n",
      "Test loss: 3.876042127609253\n",
      "Epoch 89301\n",
      "Test loss: 3.875854015350342\n",
      "Epoch 89401\n",
      "Test loss: 3.8749077320098877\n",
      "Epoch 89501\n",
      "Test loss: 3.8754851818084717\n",
      "Epoch 89601\n",
      "Test loss: 3.8748340606689453\n",
      "Epoch 89701\n",
      "Test loss: 3.8746180534362793\n",
      "Epoch 89801\n",
      "Test loss: 3.874300718307495\n",
      "Epoch 89901\n",
      "Test loss: 3.873609781265259\n",
      "Epoch 90001\n",
      "Test loss: 3.873431921005249\n",
      "Epoch 90101\n",
      "Test loss: 3.8737375736236572\n",
      "Epoch 90201\n",
      "Test loss: 3.873039960861206\n",
      "Epoch 90301\n",
      "Test loss: 3.8723766803741455\n",
      "Epoch 90401\n",
      "Test loss: 3.8725836277008057\n",
      "Epoch 90501\n",
      "Test loss: 3.871807098388672\n",
      "Epoch 90601\n",
      "Test loss: 3.871356248855591\n",
      "Epoch 90701\n",
      "Test loss: 3.870760917663574\n",
      "Epoch 90801\n",
      "Test loss: 3.8705883026123047\n",
      "Epoch 90901\n",
      "Test loss: 3.8700690269470215\n",
      "Epoch 91001\n",
      "Test loss: 3.870138168334961\n",
      "Epoch 91101\n",
      "Test loss: 3.8689866065979004\n",
      "Epoch 91201\n",
      "Test loss: 3.868959426879883\n",
      "Epoch 91301\n",
      "Test loss: 3.8687546253204346\n",
      "Epoch 91401\n",
      "Test loss: 3.867610454559326\n",
      "Epoch 91501\n",
      "Test loss: 3.8670711517333984\n",
      "Epoch 91601\n",
      "Test loss: 3.86685848236084\n",
      "Epoch 91701\n",
      "Test loss: 3.8661980628967285\n",
      "Epoch 91801\n",
      "Test loss: 3.8660483360290527\n",
      "Epoch 91901\n",
      "Test loss: 3.8651750087738037\n",
      "Epoch 92001\n",
      "Test loss: 3.86509108543396\n",
      "Epoch 92101\n",
      "Test loss: 3.864699125289917\n",
      "Epoch 92201\n",
      "Test loss: 3.8643875122070312\n",
      "Epoch 92301\n",
      "Test loss: 3.864215135574341\n",
      "Epoch 92401\n",
      "Test loss: 3.8635172843933105\n",
      "Epoch 92501\n",
      "Test loss: 3.862578868865967\n",
      "Epoch 92601\n",
      "Test loss: 3.862929582595825\n",
      "Epoch 92701\n",
      "Test loss: 3.862497568130493\n",
      "Epoch 92801\n",
      "Test loss: 3.8623273372650146\n",
      "Epoch 92901\n",
      "Test loss: 3.8617546558380127\n",
      "Epoch 93001\n",
      "Test loss: 3.861464738845825\n",
      "Epoch 93101\n",
      "Test loss: 3.860938310623169\n",
      "Epoch 93201\n",
      "Test loss: 3.86028790473938\n",
      "Epoch 93301\n",
      "Test loss: 3.860501289367676\n",
      "Epoch 93401\n",
      "Test loss: 3.8594141006469727\n",
      "Epoch 93501\n",
      "Test loss: 3.8592007160186768\n",
      "Epoch 93601\n",
      "Test loss: 3.8588788509368896\n",
      "Epoch 93701\n",
      "Test loss: 3.8590686321258545\n",
      "Epoch 93801\n",
      "Test loss: 3.8589532375335693\n",
      "Epoch 93901\n",
      "Test loss: 3.858184814453125\n",
      "Epoch 94001\n",
      "Test loss: 3.8575313091278076\n",
      "Epoch 94101\n",
      "Test loss: 3.857442617416382\n",
      "Epoch 94201\n",
      "Test loss: 3.856811761856079\n",
      "Epoch 94301\n",
      "Test loss: 3.8561055660247803\n",
      "Epoch 94401\n",
      "Test loss: 3.8554844856262207\n",
      "Epoch 94501\n",
      "Test loss: 3.854548454284668\n",
      "Epoch 94601\n",
      "Test loss: 3.854700803756714\n",
      "Epoch 94701\n",
      "Test loss: 3.854379653930664\n",
      "Epoch 94801\n",
      "Test loss: 3.853755474090576\n",
      "Epoch 94901\n",
      "Test loss: 3.853564500808716\n",
      "Epoch 95001\n",
      "Test loss: 3.852931261062622\n",
      "Epoch 95101\n",
      "Test loss: 3.8522050380706787\n",
      "Epoch 95201\n",
      "Test loss: 3.8523097038269043\n",
      "Epoch 95301\n",
      "Test loss: 3.8514766693115234\n",
      "Epoch 95401\n",
      "Test loss: 3.8512461185455322\n",
      "Epoch 95501\n",
      "Test loss: 3.8507635593414307\n",
      "Epoch 95601\n",
      "Test loss: 3.8506953716278076\n",
      "Epoch 95701\n",
      "Test loss: 3.8500571250915527\n",
      "Epoch 95801\n",
      "Test loss: 3.8505043983459473\n",
      "Epoch 95901\n",
      "Test loss: 3.8491032123565674\n",
      "Epoch 96001\n",
      "Test loss: 3.8486945629119873\n",
      "Epoch 96101\n",
      "Test loss: 3.848031759262085\n",
      "Epoch 96201\n",
      "Test loss: 3.847151517868042\n",
      "Epoch 96301\n",
      "Test loss: 3.8471031188964844\n",
      "Epoch 96401\n",
      "Test loss: 3.8467788696289062\n",
      "Epoch 96501\n",
      "Test loss: 3.8454043865203857\n",
      "Epoch 96601\n",
      "Test loss: 3.8456695079803467\n",
      "Epoch 96701\n",
      "Test loss: 3.8448331356048584\n",
      "Epoch 96801\n",
      "Test loss: 3.84462308883667\n",
      "Epoch 96901\n",
      "Test loss: 3.8443210124969482\n",
      "Epoch 97001\n",
      "Test loss: 3.8437726497650146\n",
      "Epoch 97101\n",
      "Test loss: 3.844054698944092\n",
      "Epoch 97201\n",
      "Test loss: 3.8431718349456787\n",
      "Epoch 97301\n",
      "Test loss: 3.8427979946136475\n",
      "Epoch 97401\n",
      "Test loss: 3.84287691116333\n",
      "Epoch 97501\n",
      "Test loss: 3.8423593044281006\n",
      "Epoch 97601\n",
      "Test loss: 3.841493844985962\n",
      "Epoch 97701\n",
      "Test loss: 3.8410656452178955\n",
      "Epoch 97801\n",
      "Test loss: 3.840341329574585\n",
      "Epoch 97901\n",
      "Test loss: 3.8405401706695557\n",
      "Epoch 98001\n",
      "Test loss: 3.839686632156372\n",
      "Epoch 98101\n",
      "Test loss: 3.839099168777466\n",
      "Epoch 98201\n",
      "Test loss: 3.8391308784484863\n",
      "Epoch 98301\n",
      "Test loss: 3.8384504318237305\n",
      "Epoch 98401\n",
      "Test loss: 3.8375563621520996\n",
      "Epoch 98501\n",
      "Test loss: 3.8378889560699463\n",
      "Epoch 98601\n",
      "Test loss: 3.836956262588501\n",
      "Epoch 98701\n",
      "Test loss: 3.836589813232422\n",
      "Epoch 98801\n",
      "Test loss: 3.835928440093994\n",
      "Epoch 98901\n",
      "Test loss: 3.8353383541107178\n",
      "Epoch 99001\n",
      "Test loss: 3.8350164890289307\n",
      "Epoch 99101\n",
      "Test loss: 3.8344082832336426\n",
      "Epoch 99201\n",
      "Test loss: 3.8340559005737305\n",
      "Epoch 99301\n",
      "Test loss: 3.834197759628296\n",
      "Epoch 99401\n",
      "Test loss: 3.833988666534424\n",
      "Epoch 99501\n",
      "Test loss: 3.8330538272857666\n",
      "Epoch 99601\n",
      "Test loss: 3.8323512077331543\n",
      "Epoch 99701\n",
      "Test loss: 3.8319616317749023\n",
      "Epoch 99801\n",
      "Test loss: 3.8315937519073486\n",
      "Epoch 99901\n",
      "Test loss: 3.831336259841919\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# additional training loops\n",
    "epochs = 100000\n",
    "for t in range(epochs):\n",
    "    fnapprox.train_model(batched_training_data,device)\n",
    "    if t % 100 == 0:\n",
    "        print(f\"Epoch {t+1}\")\n",
    "        loss = fnapprox.test_model(batched_test_data,device)\n",
    "        testloss.append(loss)\n",
    "        print(f\"Test loss: {loss}\\r\")\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c9ed91b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(fnapprox.state_dict(), \"fnapprox-untrained.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d4cbf571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from fnapproxto import fnapproxto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# load the trained model\n",
    "state_size = 6\n",
    "action_size = 2\n",
    "hidden_size = 31\n",
    "learning_rate = 0.0001\n",
    "trained = fnapproxto(state_size=state_size, action_size=action_size, hidden_size=hidden_size, learning_rate=learning_rate).to(device)\n",
    "trained.load_state_dict(torch.load(\"fnapprox.pth\", weights_only=True))\n",
    "\n",
    "# load a untrained model.\n",
    "untrained = fnapproxto(state_size=state_size, action_size=action_size, hidden_size=hidden_size, learning_rate=learning_rate).to(device)\n",
    "untrained.load_state_dict(torch.load(\"fnapprox-untrained.pth\", weights_only=True))\n",
    "\n",
    "\n",
    "# import csv data with pd\n",
    "data = pd.read_csv('generated_dataset_test.csv')\n",
    "mean = np.array(data).mean(axis=0, keepdims=True)\n",
    "std = np.array(data).std(axis=0, keepdims=True)\n",
    "xvarnum = 6\n",
    "for i in range(xvarnum):\n",
    "    data.iloc[:, i] = (data.iloc[:, i] - mean[0][i]) / std[0][i]\n",
    "\n",
    "# batch\n",
    "batch_num = 1\n",
    "\n",
    "# separate the data into batches then to tensors \n",
    "def create_batches(data, num_batches):\n",
    "    data = np.array(data)\n",
    "    np.random.shuffle(data)\n",
    "    batch_size = len(data) // num_batches\n",
    "    batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    return [(torch.tensor(batch[:,0:6], dtype=torch.float32),torch.tensor(batch[:,-2:], dtype=torch.float32)) for batch in batches]\n",
    "\n",
    "\n",
    "training_data = create_batches(data, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29dd785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.616389751434326\n",
      "50.58995819091797\n",
      "1234.2237548828125\n",
      "11786070.0\n"
     ]
    }
   ],
   "source": [
    "# predict using trained and untrained models\n",
    "trainedpred = trained(training_data[0][0])\n",
    "untrainedpred = untrained(training_data[0][0])\n",
    "\n",
    "# get average difference from the actual values\n",
    "traineddiff = torch.mean(torch.abs(trainedpred - training_data[0][1]))\n",
    "untraineddiff = torch.mean(torch.abs(untrainedpred - training_data[0][1]))\n",
    "traineddiffvar = torch.var(torch.abs(trainedpred - training_data[0][1]))\n",
    "untraineddiffvar = torch.var(torch.abs(untrainedpred - training_data[0][1]))\n",
    "\n",
    "print(traineddiff.item())\n",
    "print(traineddiffvar.item())\n",
    "print(untraineddiff.item())\n",
    "print(untraineddiffvar.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5fde7e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.616389751434326\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "trained_copy = copy.deepcopy(trained)\n",
    "trainedpred = trained_copy(training_data[0][0])\n",
    "traineddiff = torch.mean(torch.abs(trainedpred - training_data[0][1]))\n",
    "untraineddiff = torch.mean(torch.abs(untrainedpred - training_data[0][1]))\n",
    "\n",
    "print(traineddiff.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
